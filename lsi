# SVD

import numpy as np
from numpy.linalg import svd

# Example documents
docs = [
    "the cat sat on the mat",
    "the dog sat on the mat",
    "the cat chased the dog"
]

# Step 1: Build vocabulary
vocab = list(set(" ".join(docs).split()))
word_index = {w: i for i, w in enumerate(vocab)}

# Step 2: Build term-document matrix (counts)
td_matrix = []
for word in vocab:
    row = [d.split().count(word) for d in docs]
    td_matrix.append(row)

A = np.array(td_matrix)

print("Vocabulary:", vocab)
print("Term-Document Matrix:\n", A)

# Step 3: Singular Value Decomposition
U, s, Vt = svd(A)

Sigma = np.diag(s)

print("\nU (terms -> concepts):\n", U)
print("\nΣ (singular values):\n", Sigma)
print("\nV^T (docs -> concepts):\n", Vt)

k = 2  # latent dimension
U_k = U[:, :k]
Sigma_k = Sigma[:k, :k]
Vt_k = Vt[:k, :]

doc_vectors = np.dot(Sigma_k, Vt_k).T  # shape: (n_docs, k)
print("\nReduced Document Representations (LSI space):\n", doc_vectors)

query = "cat and dog play together"
q_vec = np.zeros((len(vocab), 1))

for word in query.lower().split():
    if word in word_index:
        q_vec[word_index[word], 0] += 1

# Project query into LSI space: q' = (q^T U_k) Σ_k^-1
q_lsi = np.dot(np.dot(q_vec.T, U_k), np.linalg.inv(Sigma_k))
print("\nQuery Representation (LSI space):\n", q_lsi)

def cosine_sim(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

print("\nSimilarity of query with each document:")
for i, doc_vec in enumerate(doc_vectors):
    sim = cosine_sim(q_lsi.flatten(), doc_vec)
    print(f"Doc{i+1}: {sim:.3f}")
